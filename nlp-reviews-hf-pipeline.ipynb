{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!cp kaggle.json /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!ls /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.64.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.14)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.28.2)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied: python-slugify in /usr/lib/python3/dist-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kaggle) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install kaggle --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## set up custom path to kaggle.json: \n",
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
    "\n",
    "from pathlib import Path\n",
    "path = Path('word2vec-nlp-tutorial')\n",
    "if not path.exists():\n",
    "    import zipfile,kaggle\n",
    "    kaggle.api.competition_download_cli(str(path))\n",
    "    # for each zip file in the path unzip it\n",
    "    for f in path.glob('*.zip'):\n",
    "        zipfile.ZipFile(f).extractall(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers as per https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv( path/\"testData.tsv\", delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4yElEQVR4nO3dfXRU1b3/8U8CyYSgkxBoMqQGmlbL85OgOAoUS0iA1IpSKpoqt0a42mCFeMHSQgxgiwR5Eqgpt1V0XahKW6kCDRlBiEoIEEkRpFTbWFrtJLdiGB5kMiTn90dvzs9peEo6IZPN+7VW1sqc/Z09e59vEj6cmUkiLMuyBAAAYJjI1l4AAABASyDkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACM1L61F9Ca6uvr9fHHH+vqq69WREREay8HAABcAsuydOLECSUnJysy8vzXa67okPPxxx8rJSWltZcBAACa4a9//auuueaa845f0SHn6quvlvTPk+R0OkMyZyAQUHFxsdLT0xUVFRWSOdF89CN80IvwQS/CB71oHp/Pp5SUFPvf8fO5okNOw1NUTqczpCEnNjZWTqeTL9gwQD/CB70IH/QifNCLf8/FXmrCC48BAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjNS+tRdgqr75W+Wvu/CfgA83Hz6Z2dpLAAAgZLiSAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBITQ45JSUluu2225ScnKyIiAht3LixUc3hw4f1zW9+U3FxcerYsaNuuOEGHT161B4/c+aMcnJy1LlzZ1111VWaMGGCqqqqguY4evSoMjMzFRsbq8TERM2cOVNnz54NqtmxY4euv/56ORwOXXvttVq7dm1TtwMAAAzV5JBz6tQpDRgwQKtXrz7n+J/+9CcNGzZMPXv21I4dO3TgwAHNnTtXMTExds2MGTP02muvacOGDdq5c6c+/vhj3XnnnfZ4XV2dMjMzVVtbq127dun555/X2rVrlZeXZ9dUVlYqMzNTt956qyoqKjR9+nQ98MAD2rp1a1O3BAAADNS+qXcYO3asxo4de97xH/3oRxo3bpwKCgrsY1/5ylfsz48fP65f/OIXWr9+vb7+9a9Lkp577jn16tVLu3fv1k033aTi4mK99957ev3115WUlKSBAwdqwYIFeuyxx5Sfn6/o6GgVFhYqNTVVS5YskST16tVLb731lpYtW6aMjIymbgsAABimySHnQurr67V582bNmjVLGRkZ2r9/v1JTUzV79myNHz9eklReXq5AIKC0tDT7fj179lS3bt1UWlqqm266SaWlperXr5+SkpLsmoyMDD300EM6dOiQBg0apNLS0qA5GmqmT59+3vX5/X75/X77ts/nkyQFAgEFAoEQnAHZ8zgirZDMdzmF6hyEk4Y9mbi3toZehA96ET7oRfNc6vkKaciprq7WyZMn9eSTT+qJJ57QokWLVFRUpDvvvFNvvPGGvva1r8nr9So6Olrx8fFB901KSpLX65Ukeb3eoIDTMN4wdqEan8+nzz77TB06dGi0voULF2revHmNjhcXFys2NrbZ+z6XBUPqQzrf5bBly5bWXkKL8Xg8rb0E/B96ET7oRfigF01z+vTpS6oL+ZUcSbr99ts1Y8YMSdLAgQO1a9cuFRYW6mtf+1ooH67JZs+erdzcXPu2z+dTSkqK0tPT5XQ6Q/IYgUBAHo9Hc/dFyl8fEZI5L5eD+eY9zdfQj9GjRysqKqq1l3NFoxfhg16ED3rRPA3PxFxMSENOly5d1L59e/Xu3TvoeMPrZSTJ5XKptrZWNTU1QVdzqqqq5HK57Jo9e/YEzdHw7qvP1/zrO7KqqqrkdDrPeRVHkhwOhxwOR6PjUVFRIf/i8tdHyF/XtkKOyd9gLdFjNA+9CB/0InzQi6a51HMV0t+TEx0drRtuuEFHjhwJOv7HP/5R3bt3lyQNHjxYUVFR2rZtmz1+5MgRHT16VG63W5Lkdrv17rvvqrq62q7xeDxyOp12gHK73UFzNNQ0zAEAAK5sTb6Sc/LkSX3wwQf27crKSlVUVCghIUHdunXTzJkzddddd2nEiBG69dZbVVRUpNdee007duyQJMXFxSk7O1u5ublKSEiQ0+nUww8/LLfbrZtuukmSlJ6ert69e+vee+9VQUGBvF6v5syZo5ycHPtKzIMPPqhVq1Zp1qxZuv/++7V9+3a9/PLL2rx5cwhOCwAAaOuaHHL27dunW2+91b7d8BqXyZMna+3atbrjjjtUWFiohQsX6vvf/7569OihX//61xo2bJh9n2XLlikyMlITJkyQ3+9XRkaGfvrTn9rj7dq106ZNm/TQQw/J7XarY8eOmjx5subPn2/XpKamavPmzZoxY4ZWrFiha665Rj//+c95+zgAAJDUjJAzcuRIWdaF3x59//336/777z/veExMjFavXn3eXygoSd27d7/ou31Gjhyp/fv3X3jBAADgisTfrgIAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGKnJIaekpES33XabkpOTFRERoY0bN5639sEHH1RERISWL18edPzYsWPKysqS0+lUfHy8srOzdfLkyaCaAwcOaPjw4YqJiVFKSooKCgoazb9hwwb17NlTMTEx6tevn7Zs2dLU7QAAAEM1OeScOnVKAwYM0OrVqy9Y98orr2j37t1KTk5uNJaVlaVDhw7J4/Fo06ZNKikp0dSpU+1xn8+n9PR0de/eXeXl5Vq8eLHy8/O1Zs0au2bXrl26++67lZ2drf3792v8+PEaP368Dh482NQtAQAAA7Vv6h3Gjh2rsWPHXrDmo48+0sMPP6ytW7cqMzMzaOzw4cMqKirS3r17NWTIEEnSypUrNW7cOD311FNKTk7WunXrVFtbq2effVbR0dHq06ePKioqtHTpUjsMrVixQmPGjNHMmTMlSQsWLJDH49GqVatUWFjY1G0BAADDNDnkXEx9fb3uvfdezZw5U3369Gk0Xlpaqvj4eDvgSFJaWpoiIyNVVlamO+64Q6WlpRoxYoSio6PtmoyMDC1atEiffvqpOnXqpNLSUuXm5gbNnZGRccGnz/x+v/x+v33b5/NJkgKBgAKBQHO3HKRhHkekFZL5LqdQnYNw0rAnE/fW1tCL8EEvwge9aJ5LPV8hDzmLFi1S+/bt9f3vf/+c416vV4mJicGLaN9eCQkJ8nq9dk1qampQTVJSkj3WqVMneb1e+9jnaxrmOJeFCxdq3rx5jY4XFxcrNjb24ptrggVD6kM63+Vg8muaPB5Pay8B/4dehA96ET7oRdOcPn36kupCGnLKy8u1YsUKvfPOO4qIiAjl1CExe/bsoKs/Pp9PKSkpSk9Pl9PpDMljBAIBeTwezd0XKX99+J2DCzmYn9HaSwi5hn6MHj1aUVFRrb2cKxq9CB/0InzQi+ZpeCbmYkIact58801VV1erW7du9rG6ujo9+uijWr58uT788EO5XC5VV1cH3e/s2bM6duyYXC6XJMnlcqmqqiqopuH2xWoaxs/F4XDI4XA0Oh4VFRXyLy5/fYT8dW0r5Jj8DdYSPUbz0IvwQS/CB71omks9VyH9PTn33nuvDhw4oIqKCvsjOTlZM2fO1NatWyVJbrdbNTU1Ki8vt++3fft21dfXa+jQoXZNSUlJ0HNuHo9HPXr0UKdOneyabdu2BT2+x+OR2+0O5ZYAAEAb1eQrOSdPntQHH3xg366srFRFRYUSEhLUrVs3de7cOag+KipKLpdLPXr0kCT16tVLY8aM0ZQpU1RYWKhAIKBp06Zp0qRJ9tvN77nnHs2bN0/Z2dl67LHHdPDgQa1YsULLli2z533kkUf0ta99TUuWLFFmZqZefPFF7du3L+ht5gAA4MrV5Cs5+/bt06BBgzRo0CBJUm5urgYNGqS8vLxLnmPdunXq2bOnRo0apXHjxmnYsGFB4SQuLk7FxcWqrKzU4MGD9eijjyovLy/od+ncfPPNWr9+vdasWaMBAwboV7/6lTZu3Ki+ffs2dUsAAMBATb6SM3LkSFnWpb89+sMPP2x0LCEhQevXr7/g/fr3768333zzgjUTJ07UxIkTL3ktAADgysHfrgIAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGKnJIaekpES33XabkpOTFRERoY0bN9pjgUBAjz32mPr166eOHTsqOTlZ9913nz7++OOgOY4dO6asrCw5nU7Fx8crOztbJ0+eDKo5cOCAhg8frpiYGKWkpKigoKDRWjZs2KCePXsqJiZG/fr105YtW5q6HQAAYKgmh5xTp05pwIABWr16daOx06dP65133tHcuXP1zjvv6De/+Y2OHDmib37zm0F1WVlZOnTokDwejzZt2qSSkhJNnTrVHvf5fEpPT1f37t1VXl6uxYsXKz8/X2vWrLFrdu3apbvvvlvZ2dnav3+/xo8fr/Hjx+vgwYNN3RIAADBQ+6beYezYsRo7duw5x+Li4uTxeIKOrVq1SjfeeKOOHj2qbt266fDhwyoqKtLevXs1ZMgQSdLKlSs1btw4PfXUU0pOTta6detUW1urZ599VtHR0erTp48qKiq0dOlSOwytWLFCY8aM0cyZMyVJCxYskMfj0apVq1RYWNjUbQEAAMM0OeQ01fHjxxUREaH4+HhJUmlpqeLj4+2AI0lpaWmKjIxUWVmZ7rjjDpWWlmrEiBGKjo62azIyMrRo0SJ9+umn6tSpk0pLS5Wbmxv0WBkZGUFPn/0rv98vv99v3/b5fJL++TRbIBAIwW5lz+OItEIy3+UUqnMQThr2ZOLe2hp6ET7oRfigF81zqeerRUPOmTNn9Nhjj+nuu++W0+mUJHm9XiUmJgYvon17JSQkyOv12jWpqalBNUlJSfZYp06d5PV67WOfr2mY41wWLlyoefPmNTpeXFys2NjYpm/wAhYMqQ/pfJeDya9p+tcrjGg99CJ80IvwQS+a5vTp05dU12IhJxAI6Nvf/rYsy9IzzzzTUg/TJLNnzw66+uPz+ZSSkqL09HQ7hP27AoGAPB6P5u6LlL8+IiRzXi4H8zNaewkh19CP0aNHKyoqqrWXc0WjF+GDXoQPetE8Dc/EXEyLhJyGgPOXv/xF27dvDwoQLpdL1dXVQfVnz57VsWPH5HK57JqqqqqgmobbF6tpGD8Xh8Mhh8PR6HhUVFTIv7j89RHy17WtkGPyN1hL9BjNQy/CB70IH/SiaS71XIX89+Q0BJz3339fr7/+ujp37hw07na7VVNTo/LycvvY9u3bVV9fr6FDh9o1JSUlQc+5eTwe9ejRQ506dbJrtm3bFjS3x+OR2+0O9ZYAAEAb1OSQc/LkSVVUVKiiokKSVFlZqYqKCh09elSBQEDf+ta3tG/fPq1bt051dXXyer3yer2qra2VJPXq1UtjxozRlClTtGfPHr399tuaNm2aJk2apOTkZEnSPffco+joaGVnZ+vQoUN66aWXtGLFiqCnmh555BEVFRVpyZIl+sMf/qD8/Hzt27dP06ZNC8FpAQAAbV2TQ86+ffs0aNAgDRo0SJKUm5urQYMGKS8vTx999JFeffVV/e1vf9PAgQPVtWtX+2PXrl32HOvWrVPPnj01atQojRs3TsOGDQv6HThxcXEqLi5WZWWlBg8erEcffVR5eXlBv0vn5ptv1vr167VmzRoNGDBAv/rVr7Rx40b17dv33zkfAADAEE1+Tc7IkSNlWed/e/SFxhokJCRo/fr1F6zp37+/3nzzzQvWTJw4URMnTrzo4wEAgCsPf7sKAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBITQ45JSUluu2225ScnKyIiAht3LgxaNyyLOXl5alr167q0KGD0tLS9P777wfVHDt2TFlZWXI6nYqPj1d2drZOnjwZVHPgwAENHz5cMTExSklJUUFBQaO1bNiwQT179lRMTIz69eunLVu2NHU7AADAUE0OOadOndKAAQO0evXqc44XFBTo6aefVmFhocrKytSxY0dlZGTozJkzdk1WVpYOHTokj8ejTZs2qaSkRFOnTrXHfT6f0tPT1b17d5WXl2vx4sXKz8/XmjVr7Jpdu3bp7rvvVnZ2tvbv36/x48dr/PjxOnjwYFO3BAAADNS+qXcYO3asxo4de84xy7K0fPlyzZkzR7fffrsk6YUXXlBSUpI2btyoSZMm6fDhwyoqKtLevXs1ZMgQSdLKlSs1btw4PfXUU0pOTta6detUW1urZ599VtHR0erTp48qKiq0dOlSOwytWLFCY8aM0cyZMyVJCxYskMfj0apVq1RYWNiskwEAAMwR0tfkVFZWyuv1Ki0tzT4WFxenoUOHqrS0VJJUWlqq+Ph4O+BIUlpamiIjI1VWVmbXjBgxQtHR0XZNRkaGjhw5ok8//dSu+fzjNNQ0PA4AALiyNflKzoV4vV5JUlJSUtDxpKQke8zr9SoxMTF4Ee3bKyEhIagmNTW10RwNY506dZLX673g45yL3++X3++3b/t8PklSIBBQIBC45H1eSMM8jkgrJPNdTqE6B+GkYU8m7q2toRfhg16ED3rRPJd6vkIacsLdwoULNW/evEbHi4uLFRsbG9LHWjCkPqTzXQ4mv3Db4/G09hLwf+hF+KAX4YNeNM3p06cvqS6kIcflckmSqqqq1LVrV/t4VVWVBg4caNdUV1cH3e/s2bM6duyYfX+Xy6WqqqqgmobbF6tpGD+X2bNnKzc3177t8/mUkpKi9PR0OZ3Opmz1vAKBgDwej+bui5S/PiIkc14uB/MzWnsJIdfQj9GjRysqKqq1l3NFoxfhg16ED3rRPA3PxFxMSENOamqqXC6Xtm3bZocan8+nsrIyPfTQQ5Ikt9utmpoalZeXa/DgwZKk7du3q76+XkOHDrVrfvSjHykQCNhN93g86tGjhzp16mTXbNu2TdOnT7cf3+PxyO12n3d9DodDDoej0fGoqKiQf3H56yPkr2tbIcfkb7CW6DGah16ED3oRPuhF01zquWryC49PnjypiooKVVRUSPrni40rKip09OhRRUREaPr06XriiSf06quv6t1339V9992n5ORkjR8/XpLUq1cvjRkzRlOmTNGePXv09ttva9q0aZo0aZKSk5MlSffcc4+io6OVnZ2tQ4cO6aWXXtKKFSuCrsI88sgjKioq0pIlS/SHP/xB+fn52rdvn6ZNm9bULQEAAAM1+UrOvn37dOutt9q3G4LH5MmTtXbtWs2aNUunTp3S1KlTVVNTo2HDhqmoqEgxMTH2fdatW6dp06Zp1KhRioyM1IQJE/T000/b43FxcSouLlZOTo4GDx6sLl26KC8vL+h36dx8881av3695syZox/+8Ie67rrrtHHjRvXt27dZJwIAAJilySFn5MiRsqzzv3MoIiJC8+fP1/z5889bk5CQoPXr11/wcfr3768333zzgjUTJ07UxIkTL7xgAABwReJvVwEAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjBTykFNXV6e5c+cqNTVVHTp00Fe+8hUtWLBAlmXZNZZlKS8vT127dlWHDh2Ulpam999/P2ieY8eOKSsrS06nU/Hx8crOztbJkyeDag4cOKDhw4crJiZGKSkpKigoCPV2AABAGxXykLNo0SI988wzWrVqlQ4fPqxFixapoKBAK1eutGsKCgr09NNPq7CwUGVlZerYsaMyMjJ05swZuyYrK0uHDh2Sx+PRpk2bVFJSoqlTp9rjPp9P6enp6t69u8rLy7V48WLl5+drzZo1od4SAABog9qHesJdu3bp9ttvV2ZmpiTpS1/6kn75y19qz549kv55FWf58uWaM2eObr/9dknSCy+8oKSkJG3cuFGTJk3S4cOHVVRUpL1792rIkCGSpJUrV2rcuHF66qmnlJycrHXr1qm2tlbPPvusoqOj1adPH1VUVGjp0qVBYQgAAFyZQh5ybr75Zq1Zs0Z//OMf9dWvflW///3v9dZbb2np0qWSpMrKSnm9XqWlpdn3iYuL09ChQ1VaWqpJkyaptLRU8fHxdsCRpLS0NEVGRqqsrEx33HGHSktLNWLECEVHR9s1GRkZWrRokT799FN16tSp0dr8fr/8fr992+fzSZICgYACgUBI9t8wjyPSukhl+AnVOQgnDXsycW9tDb0IH/QifNCL5rnU8xXykPODH/xAPp9PPXv2VLt27VRXV6cf//jHysrKkiR5vV5JUlJSUtD9kpKS7DGv16vExMTghbZvr4SEhKCa1NTURnM0jJ0r5CxcuFDz5s1rdLy4uFixsbHN2e55LRhSH9L5LoctW7a09hJajMfjae0l4P/Qi/BBL8IHvWia06dPX1JdyEPOyy+/rHXr1mn9+vX2U0jTp09XcnKyJk+eHOqHa5LZs2crNzfXvu3z+ZSSkqL09HQ5nc6QPEYgEJDH49HcfZHy10eEZM7L5WB+RmsvIeQa+jF69GhFRUW19nKuaPQifNCL8EEvmqfhmZiLCXnImTlzpn7wgx9o0qRJkqR+/frpL3/5ixYuXKjJkyfL5XJJkqqqqtS1a1f7flVVVRo4cKAkyeVyqbq6Omjes2fP6tixY/b9XS6XqqqqgmoabjfU/CuHwyGHw9HoeFRUVMi/uPz1EfLXta2QY/I3WEv0GM1DL8IHvQgf9KJpLvVchfzdVadPn1ZkZPC07dq1U339P5++SU1Nlcvl0rZt2+xxn8+nsrIyud1uSZLb7VZNTY3Ky8vtmu3bt6u+vl5Dhw61a0pKSoKel/N4POrRo8c5n6oCAABXlpCHnNtuu00//vGPtXnzZn344Yd65ZVXtHTpUt1xxx2SpIiICE2fPl1PPPGEXn31Vb377ru67777lJycrPHjx0uSevXqpTFjxmjKlCnas2eP3n77bU2bNk2TJk1ScnKyJOmee+5RdHS0srOzdejQIb300ktasWJF0NNRAADgyhXyp6tWrlypuXPn6nvf+56qq6uVnJys//zP/1ReXp5dM2vWLJ06dUpTp05VTU2Nhg0bpqKiIsXExNg169at07Rp0zRq1ChFRkZqwoQJevrpp+3xuLg4FRcXKycnR4MHD1aXLl2Ul5fH28cBAICkFgg5V199tZYvX67ly5eftyYiIkLz58/X/Pnzz1uTkJCg9evXX/Cx+vfvrzfffLO5SwUAAAbjb1cBAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIzUIiHno48+0ne+8x117txZHTp0UL9+/bRv3z573LIs5eXlqWvXrurQoYPS0tL0/vvvB81x7NgxZWVlyel0Kj4+XtnZ2Tp58mRQzYEDBzR8+HDFxMQoJSVFBQUFLbEdAADQBoU85Hz66ae65ZZbFBUVpd/97nd67733tGTJEnXq1MmuKSgo0NNPP63CwkKVlZWpY8eOysjI0JkzZ+yarKwsHTp0SB6PR5s2bVJJSYmmTp1qj/t8PqWnp6t79+4qLy/X4sWLlZ+frzVr1oR6SwAAoA1qH+oJFy1apJSUFD333HP2sdTUVPtzy7K0fPlyzZkzR7fffrsk6YUXXlBSUpI2btyoSZMm6fDhwyoqKtLevXs1ZMgQSdLKlSs1btw4PfXUU0pOTta6detUW1urZ599VtHR0erTp48qKiq0dOnSoDAEAACuTCEPOa+++qoyMjI0ceJE7dy5U1/84hf1ve99T1OmTJEkVVZWyuv1Ki0tzb5PXFychg4dqtLSUk2aNEmlpaWKj4+3A44kpaWlKTIyUmVlZbrjjjtUWlqqESNGKDo62q7JyMjQokWL9OmnnwZdOWrg9/vl9/vt2z6fT5IUCAQUCARCsv+GeRyRVkjmu5xCdQ7CScOeTNxbW0Mvwge9CB/0onku9XyFPOT8+c9/1jPPPKPc3Fz98Ic/1N69e/X9739f0dHRmjx5srxeryQpKSkp6H5JSUn2mNfrVWJiYvBC27dXQkJCUM3nrxB9fk6v13vOkLNw4ULNmzev0fHi4mLFxsY2c8fntmBIfUjnuxy2bNnS2ktoMR6Pp7WXgP9DL8IHvQgf9KJpTp8+fUl1IQ859fX1GjJkiH7yk59IkgYNGqSDBw+qsLBQkydPDvXDNcns2bOVm5tr3/b5fEpJSVF6erqcTmdIHiMQCMjj8Wjuvkj56yNCMuflcjA/o7WXEHIN/Rg9erSioqJaezlXNHoRPuhF+KAXzdPwTMzFhDzkdO3aVb179w461qtXL/3617+WJLlcLklSVVWVunbtatdUVVVp4MCBdk11dXXQHGfPntWxY8fs+7tcLlVVVQXVNNxuqPlXDodDDoej0fGoqKiQf3H56yPkr2tbIcfkb7CW6DGah16ED3oRPuhF01zquQr5u6tuueUWHTlyJOjYH//4R3Xv3l3SP1+E7HK5tG3bNnvc5/OprKxMbrdbkuR2u1VTU6Py8nK7Zvv27aqvr9fQoUPtmpKSkqDn5Twej3r06HHOp6oAAMCVJeRXcmbMmKGbb75ZP/nJT/Ttb39be/bs0Zo1a+y3dkdERGj69Ol64okndN111yk1NVVz585VcnKyxo8fL+mfV37GjBmjKVOmqLCwUIFAQNOmTdOkSZOUnJwsSbrnnns0b948ZWdn67HHHtPBgwe1YsUKLVu2LNRbumJ86QebW3sJTfbhk5mtvQQAQJgKeci54YYb9Morr2j27NmaP3++UlNTtXz5cmVlZdk1s2bN0qlTpzR16lTV1NRo2LBhKioqUkxMjF2zbt06TZs2TaNGjVJkZKQmTJigp59+2h6Pi4tTcXGxcnJyNHjwYHXp0kV5eXm8fRwAAEhqgZAjSd/4xjf0jW9847zjERERmj9/vubPn3/emoSEBK1fv/6Cj9O/f3+9+eabzV4nAAAwF3+7CgAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgpBYPOU8++aQiIiI0ffp0+9iZM2eUk5Ojzp0766qrrtKECRNUVVUVdL+jR48qMzNTsbGxSkxM1MyZM3X27Nmgmh07duj666+Xw+HQtddeq7Vr17b0dgAAQBvRoiFn7969+tnPfqb+/fsHHZ8xY4Zee+01bdiwQTt37tTHH3+sO++80x6vq6tTZmamamtrtWvXLj3//PNau3at8vLy7JrKykplZmbq1ltvVUVFhaZPn64HHnhAW7dubcktAQCANqLFQs7JkyeVlZWl//7v/1anTp3s48ePH9cvfvELLV26VF//+tc1ePBgPffcc9q1a5d2794tSSouLtZ7772n//mf/9HAgQM1duxYLViwQKtXr1Ztba0kqbCwUKmpqVqyZIl69eqladOm6Vvf+paWLVvWUlsCAABtSPuWmjgnJ0eZmZlKS0vTE088YR8vLy9XIBBQWlqafaxnz57q1q2bSktLddNNN6m0tFT9+vVTUlKSXZORkaGHHnpIhw4d0qBBg1RaWho0R0PN558W+1d+v19+v9++7fP5JEmBQECBQODf3bI9lyQ5Iq2QzIcLu1jfGsZD1V80H70IH/QifNCL5rnU89UiIefFF1/UO++8o7179zYa83q9io6OVnx8fNDxpKQkeb1eu+bzAadhvGHsQjU+n0+fffaZOnTo0OixFy5cqHnz5jU6XlxcrNjY2Evf4CVYMKQ+pPPh3LZs2XJJdR6Pp4VXgktFL8IHvQgf9KJpTp8+fUl1IQ85f/3rX/XII4/I4/EoJiYm1NP/W2bPnq3c3Fz7ts/nU0pKitLT0+V0OkPyGIFAQB6PR3P3RcpfHxGSOXF+B/MzLjje0I/Ro0crKirqMq0K50Ivwge9CB/0onkanom5mJCHnPLyclVXV+v666+3j9XV1amkpESrVq3S1q1bVVtbq5qamqCrOVVVVXK5XJIkl8ulPXv2BM3b8O6rz9f86zuyqqqq5HQ6z3kVR5IcDoccDkej41FRUSH/4vLXR8hfR8hpaZfat5boMZqHXoQPehE+6EXTXOq5CvkLj0eNGqV3331XFRUV9seQIUOUlZVlfx4VFaVt27bZ9zly5IiOHj0qt9stSXK73Xr33XdVXV1t13g8HjmdTvXu3duu+fwcDTUNcwAAgCtbyK/kXH311erbt2/QsY4dO6pz58728ezsbOXm5iohIUFOp1MPP/yw3G63brrpJklSenq6evfurXvvvVcFBQXyer2aM2eOcnJy7CsxDz74oFatWqVZs2bp/vvv1/bt2/Xyyy9r8+bNod4SAABog1rs3VUXsmzZMkVGRmrChAny+/3KyMjQT3/6U3u8Xbt22rRpkx566CG53W517NhRkydP1vz58+2a1NRUbd68WTNmzNCKFSt0zTXX6Oc//7kyMi78Gg0AAHBluCwhZ8eOHUG3Y2JitHr1aq1evfq89+nevftF3zkzcuRI7d+/PxRLBAAAhuFvVwEAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGCnkIWfhwoW64YYbdPXVVysxMVHjx4/XkSNHgmrOnDmjnJwcde7cWVdddZUmTJigqqqqoJqjR48qMzNTsbGxSkxM1MyZM3X27Nmgmh07duj666+Xw+HQtddeq7Vr14Z6OwAAoI0KecjZuXOncnJytHv3bnk8HgUCAaWnp+vUqVN2zYwZM/Taa69pw4YN2rlzpz7++GPdeeed9nhdXZ0yMzNVW1urXbt26fnnn9fatWuVl5dn11RWViozM1O33nqrKioqNH36dD3wwAPaunVrqLcEAADaoPahnrCoqCjo9tq1a5WYmKjy8nKNGDFCx48f1y9+8QutX79eX//61yVJzz33nHr16qXdu3frpptuUnFxsd577z29/vrrSkpK0sCBA7VgwQI99thjys/PV3R0tAoLC5WamqolS5ZIknr16qW33npLy5YtU0ZGRqi3BQAA2pgWf03O8ePHJUkJCQmSpPLycgUCAaWlpdk1PXv2VLdu3VRaWipJKi0tVb9+/ZSUlGTXZGRkyOfz6dChQ3bN5+doqGmYAwAAXNlCfiXn8+rr6zV9+nTdcsst6tu3ryTJ6/UqOjpa8fHxQbVJSUnyer12zecDTsN4w9iFanw+nz777DN16NCh0Xr8fr/8fr992+fzSZICgYACgcC/sdP/r2EeR6QVkvlwYRfrW8N4qPqL5qMX4YNehA960TyXer5aNOTk5OTo4MGDeuutt1ryYS7ZwoULNW/evEbHi4uLFRsbG9LHWjCkPqTz4dy2bNlySXUej6eFV4JLRS/CB70IH/SiaU6fPn1JdS0WcqZNm6ZNmzappKRE11xzjX3c5XKptrZWNTU1QVdzqqqq5HK57Jo9e/YEzdfw7qvP1/zrO7KqqqrkdDrPeRVHkmbPnq3c3Fz7ts/nU0pKitLT0+V0Opu/2c8JBALyeDyauy9S/vqIkMyJ8zuYf+HXXzX0Y/To0YqKirpMq8K50IvwQS/CB71onoZnYi4m5CHHsiw9/PDDeuWVV7Rjxw6lpqYGjQ8ePFhRUVHatm2bJkyYIEk6cuSIjh49KrfbLUlyu9368Y9/rOrqaiUmJkr6Z8p1Op3q3bu3XfOv/4v3eDz2HOficDjkcDgaHY+Kigr5F5e/PkL+OkJOS7vUvrVEj9E89CJ80IvwQS+a5lLPVchDTk5OjtavX6/f/va3uvrqq+3X0MTFxalDhw6Ki4tTdna2cnNzlZCQIKfTqYcfflhut1s33XSTJCk9PV29e/fWvffeq4KCAnm9Xs2ZM0c5OTl2SHnwwQe1atUqzZo1S/fff7+2b9+ul19+WZs3bw71lgAAQBsU8ndXPfPMMzp+/LhGjhyprl272h8vvfSSXbNs2TJ94xvf0IQJEzRixAi5XC795je/scfbtWunTZs2qV27dnK73frOd76j++67T/Pnz7drUlNTtXnzZnk8Hg0YMEBLlizRz3/+c94+DgAAJLXQ01UXExMTo9WrV2v16tXnrenevftFX1Q6cuRI7d+/v8lrBAAA5uNvVwEAACMRcgAAgJEIOQAAwEgt+ssAgZb2pR9c+N10jnaWCm6U+uZvDau39H/4ZGZrLwEAjMeVHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIzUvrUXAFyJvvSDza29hCb78MnM1l4CADQJV3IAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIzU5t9Cvnr1ai1evFher1cDBgzQypUrdeONN7b2sgDj/Ltve3e0s1Rwo9Q3f6v8dREhWtWF8bZ34MrWpq/kvPTSS8rNzdXjjz+ud955RwMGDFBGRoaqq6tbe2kAAKCVtemQs3TpUk2ZMkXf/e531bt3bxUWFio2NlbPPvtsay8NAAC0sjb7dFVtba3Ky8s1e/Zs+1hkZKTS0tJUWlp6zvv4/X75/X779vHjxyVJx44dUyAQCMm6AoGATp8+rfaBSNXVX55L8ji/9vWWTp+upx9hoDV68cknn1yWx2lrGn5OffLJJ4qKimrt5VzR6EXznDhxQpJkWdYF69psyPnHP/6huro6JSUlBR1PSkrSH/7wh3PeZ+HChZo3b16j46mpqS2yRoSHe1p7AbBd7l50WXKZHxDAZXXixAnFxcWdd7zNhpzmmD17tnJzc+3b9fX1OnbsmDp37qyIiND8z9Ln8yklJUV//etf5XQ6QzInmo9+hA96ET7oRfigF81jWZZOnDih5OTkC9a12ZDTpUsXtWvXTlVVVUHHq6qq5HK5znkfh8Mhh8MRdCw+Pr5F1ud0OvmCDSP0I3zQi/BBL8IHvWi6C13BadBmX3gcHR2twYMHa9u2bfax+vp6bdu2TW63uxVXBgAAwkGbvZIjSbm5uZo8ebKGDBmiG2+8UcuXL9epU6f03e9+t7WXBgAAWlmbDjl33XWX/vd//1d5eXnyer0aOHCgioqKGr0Y+XJyOBx6/PHHGz0thtZBP8IHvQgf9CJ80IuWFWFd7P1XAAAAbVCbfU0OAADAhRByAACAkQg5AADASIQcAABgJEJOiK1evVpf+tKXFBMTo6FDh2rPnj2tvSTj5OfnKyIiIuijZ8+e9viZM2eUk5Ojzp0766qrrtKECRMa/dLIo0ePKjMzU7GxsUpMTNTMmTN19uzZy72VNqekpES33XabkpOTFRERoY0bNwaNW5alvLw8de3aVR06dFBaWpref//9oJpjx44pKytLTqdT8fHxys7O1smTJ4NqDhw4oOHDhysmJkYpKSkqKCho6a21ORfrxX/8x380+j4ZM2ZMUA29CI2FCxfqhhtu0NVXX63ExESNHz9eR44cCaoJ1c+lHTt26Prrr5fD4dC1116rtWvXtvT22jRCTgi99NJLys3N1eOPP6533nlHAwYMUEZGhqqrq1t7acbp06eP/v73v9sfb731lj02Y8YMvfbaa9qwYYN27typjz/+WHfeeac9XldXp8zMTNXW1mrXrl16/vnntXbtWuXl5bXGVtqUU6dOacCAAVq9evU5xwsKCvT000+rsLBQZWVl6tixozIyMnTmzBm7JisrS4cOHZLH49GmTZtUUlKiqVOn2uM+n0/p6enq3r27ysvLtXjxYuXn52vNmjUtvr+25GK9kKQxY8YEfZ/88pe/DBqnF6Gxc+dO5eTkaPfu3fJ4PAoEAkpPT9epU6fsmlD8XKqsrFRmZqZuvfVWVVRUaPr06XrggQe0devWy7rfNsVCyNx4441WTk6Ofbuurs5KTk62Fi5c2IqrMs/jjz9uDRgw4JxjNTU1VlRUlLVhwwb72OHDhy1JVmlpqWVZlrVlyxYrMjLS8nq9ds0zzzxjOZ1Oy+/3t+jaTSLJeuWVV+zb9fX1lsvlshYvXmwfq6mpsRwOh/XLX/7SsizLeu+99yxJ1t69e+2a3/3ud1ZERIT10UcfWZZlWT/96U+tTp06BfXiscces3r06NHCO2q7/rUXlmVZkydPtm6//fbz3odetJzq6mpLkrVz507LskL3c2nWrFlWnz59gh7rrrvusjIyMlp6S20WV3JCpLa2VuXl5UpLS7OPRUZGKi0tTaWlpa24MjO9//77Sk5O1pe//GVlZWXp6NGjkqTy8nIFAoGgPvTs2VPdunWz+1BaWqp+/foF/dLIjIwM+Xw+HTp06PJuxCCVlZXyer1B5z4uLk5Dhw4NOvfx8fEaMmSIXZOWlqbIyEiVlZXZNSNGjFB0dLRdk5GRoSNHjujTTz+9TLsxw44dO5SYmKgePXrooYce0ieffGKP0YuWc/z4cUlSQkKCpND9XCotLQ2ao6GGf2POj5ATIv/4xz9UV1fX6LctJyUlyev1ttKqzDR06FCtXbtWRUVFeuaZZ1RZWanhw4frxIkT8nq9io6ObvSHVz/fB6/Xe84+NYyheRrO3YW+B7xerxITE4PG27dvr4SEBPoTYmPGjNELL7ygbdu2adGiRdq5c6fGjh2ruro6SfSipdTX12v69Om65ZZb1LdvX0kK2c+l89X4fD599tlnLbGdNq9N/1kHXJnGjh1rf96/f38NHTpU3bt318svv6wOHTq04sqA8DFp0iT78379+ql///76yle+oh07dmjUqFGtuDKz5eTk6ODBg0GvE0Tr4UpOiHTp0kXt2rVr9Gr5qqoquVyuVlrVlSE+Pl5f/epX9cEHH8jlcqm2tlY1NTVBNZ/vg8vlOmefGsbQPA3n7kLfAy6Xq9EL8c+ePatjx47Rnxb25S9/WV26dNEHH3wgiV60hGnTpmnTpk164403dM0119jHQ/Vz6Xw1TqeT/+CdByEnRKKjozV48GBt27bNPlZfX69t27bJ7Xa34srMd/LkSf3pT39S165dNXjwYEVFRQX14ciRIzp69KjdB7fbrXfffTfoB7zH45HT6VTv3r0v+/pNkZqaKpfLFXTufT6fysrKgs59TU2NysvL7Zrt27ervr5eQ4cOtWtKSkoUCATsGo/Hox49eqhTp06XaTfm+dvf/qZPPvlEXbt2lUQvQsmyLE2bNk2vvPKKtm/frtTU1KDxUP1ccrvdQXM01PBvzAW09iufTfLiiy9aDofDWrt2rfXee+9ZU6dOteLj44NeLY9/36OPPmrt2LHDqqystN5++20rLS3N6tKli1VdXW1ZlmU9+OCDVrdu3azt27db+/bts9xut+V2u+37nz171urbt6+Vnp5uVVRUWEVFRdYXvvAFa/bs2a21pTbjxIkT1v79+639+/dbkqylS5da+/fvt/7yl79YlmVZTz75pBUfH2/99re/tQ4cOGDdfvvtVmpqqvXZZ5/Zc4wZM8YaNGiQVVZWZr311lvWddddZ9199932eE1NjZWUlGTde++91sGDB60XX3zRio2NtX72s59d9v2Gswv14sSJE9Z//dd/WaWlpVZlZaX1+uuvW9dff7113XXXWWfOnLHnoBeh8dBDD1lxcXHWjh07rL///e/2x+nTp+2aUPxc+vOf/2zFxsZaM2fOtA4fPmytXr3aateunVVUVHRZ99uWEHJCbOXKlVa3bt2s6Oho68Ybb7R2797d2ksyzl133WV17drVio6Otr74xS9ad911l/XBBx/Y45999pn1ve99z+rUqZMVGxtr3XHHHdbf//73oDk+/PBDa+zYsVaHDh2sLl26WI8++qgVCAQu91banDfeeMOS1Ohj8uTJlmX9823kc+fOtZKSkiyHw2GNGjXKOnLkSNAcn3zyiXX33XdbV111leV0Oq3vfve71okTJ4Jqfv/731vDhg2zHA6H9cUvftF68sknL9cW24wL9eL06dNWenq69YUvfMGKioqyunfvbk2ZMqXRf7joRWicqw+SrOeee86uCdXPpTfeeMMaOHCgFR0dbX35y18Oegw0FmFZlnW5rx4BAAC0NF6TAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICR/h8eTF6iArNohAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = test.review.apply(lambda x: len(x.split()))\n",
    "lengths.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1885, 0.0754)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len500 = (lengths > 500).sum()\n",
    "len500, len500 / len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'label': 'POSITIVE', 'score': 0.9996938705444336}],\n",
       " '\"Naturally in a film who\\'s main themes are of mortality, nostalgia, and loss of innocence it is perhaps not surprising that it is rated more highly by older viewers than younger ones. However there is a craftsmanship and completeness to the film which anyone can enjoy. The pace is steady and constant, the characters full and engaging, the relationships and interactions natural showing that you do not need floods of tears to show emotion, screams to show fear, shouting to show dispute or violence to show anger. Naturally Joyce\\'s short story lends the film a ready made structure as perfect as a polished diamond, but the small changes Huston makes such as the inclusion of the poem fit in neatly. It is truly a masterpiece of tact, subtlety and overwhelming beauty.\"')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(test.review[0]), test.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996938705444336},\n",
       " {'label': 'NEGATIVE', 'score': 0.9977385997772217},\n",
       " {'label': 'POSITIVE', 'score': 0.8819959759712219}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(list(test.review[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# preds = classifier(list(test.review)) ## does not work with all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.21.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache\n",
      "Location: /usr/local/lib/python3.9/dist-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, tokenizers, tqdm\n",
      "Required-by: sentence-transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'summary_text': ' There is a craftsmanship and completeness to the film which anyone can enjoy . The pace is steady and constant, the characters full and engaging, the relationships and interactions natural showing that you do not need floods of tears to show emotion, screams to show fear, shouting to show dispute or violence to show anger .'}],\n",
       " '\"Naturally in a film who\\'s main themes are of mortality, nostalgia, and loss of innocence it is perhaps not surprising that it is rated more highly by older viewers than younger ones. However there is a craftsmanship and completeness to the film which anyone can enjoy. The pace is steady and constant, the characters full and engaging, the relationships and interactions natural showing that you do not need floods of tears to show emotion, screams to show fear, shouting to show dispute or violence to show anger. Naturally Joyce\\'s short story lends the film a ready made structure as perfect as a polished diamond, but the small changes Huston makes such as the inclusion of the poem fit in neatly. It is truly a masterpiece of tact, subtlety and overwhelming beauty.\"')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(test.review[0]), test.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 76. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summarized_reviews = summarizer(list(test.review[:10])) # 3 sec per example. for ~2000 examples it will take ~1.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' There is a craftsmanship and completeness to the film which anyone can enjoy . The pace is steady and constant, the characters full and engaging, the relationships and interactions natural showing that you do not need floods of tears to show emotion, screams to show fear, shouting to show dispute or violence to show anger .'},\n",
       " {'summary_text': ' \"This movie is a disaster within a disaster film. It is full of great action scenes, which are only meaningful if you throw away all sense of reality\" \"Hard to believe somebody read the scripts for this and allowed all this talent to be wasted,\" says Tom Charity .'},\n",
       " {'summary_text': ' \"All in all, this is a movie for kids. At one point my kid\\'s excitement was so great that sitting was impossible\" \"I am a great fan of A.A. Milne\\'s books which are very subtle and hide a wry intelligence behind the childlike quality of its leading characters. This film was not subtle. It seems a shame that Disney cannot see the benefit of making movies from more of the stories contained in those pages .'},\n",
       " {'summary_text': ' At his best, the protagonist, Lucas, is creepy. As hard as it is to draw a bead on the secondary characters, they\\'re far more sympathetic . \"Afraid of the Dark\" could have achieved mediocrity had it taken just one approach and seen it through .'},\n",
       " {'summary_text': ' \"A very accurate depiction of small time mob life filmed in New Jersey\" \"The story, characters and script are believable but the acting drops the ball\" \"A young hood steps up and starts doing bigger things (tries to) but these things keep going wrong, leading the local boss to suspect that his end is being skimmed off\"'},\n",
       " {'summary_text': \" The film offers a glimpse into the lifestyles of Egypt's poor as well as its elite . While errors abound on the large scale, the details are more meticulously researched than the vast majority of Hollywood's films . Visually, it's not without its flaws - the interiors are often too overly lit and colourful to blend seamlessly with the exteriors .\"},\n",
       " {'summary_text': ' \"This has to be one of the biggest misfires ever...the script was nice and could have ended a lot better\" \"If i had to re-watch it it would be like torture. The actors should have played better and maybe then i would have given this movie a slightly better grade\"'},\n",
       " {'summary_text': ' \"This is one of those movies I watched, and wondered, why did I watch it? What did I find so interesting about it? Being a truck driver myself, I didn\\'t find it very realistic. No, I\\'ve never used a \\'lot lizard\\', nor have I ever seen, nor heard about one traveling around the country in a brand new seventy thousand dollar RV, either .'},\n",
       " {'summary_text': ' \"The worst movie i\\'ve seen in years,\" says one fan . \"There is no plot whatsoever, there is no point whatsoever, i felt robbed after i rented this movie,\" she says . \"A disgrace for terrible movies! stay away from this terrible piece of c**p. save your money\"'},\n",
       " {'summary_text': ' Kevin Bacon, William Baldwin, Julia Roberts & Kiefer Sutherland were in the early stages of their adult acting careers . \"Flatliners\" is now an all-star cult semi-sci-fi suspense . \"Who knew 17 years ago that the film careers of this young group of actors would skyrocket? I suspect that director Joel Schumacher did.\"'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     There is a craftsmanship and completeness to ...\n",
       "1     \"This movie is a disaster within a disaster f...\n",
       "2     \"All in all, this is a movie for kids. At one...\n",
       "Name: summary_text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_reviews = pd.DataFrame(summarized_reviews)\n",
    "summarized_reviews.summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# so lets just cut the reviews to 500 words and then do sentiment analysis\n",
    "test['review'] = test.review.apply(lambda x: ' '.join(x.split()[:300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\projects\\studying-dl\\nlp-reviews-hf-pipeline.ipynb Cell 21\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/projects/studying-dl/nlp-reviews-hf-pipeline.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m preds \u001b[39m=\u001b[39m classifier(\u001b[39mlist\u001b[39;49m(test\u001b[39m.\u001b[39;49mreview))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_classification.py:138\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    105\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    139\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1056\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1053\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1054\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1055\u001b[0m     )\n\u001b[0;32m-> 1056\u001b[0m     outputs \u001b[39m=\u001b[39m [output \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_iterator]\n\u001b[1;32m   1057\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1058\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1056\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1053\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1054\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1055\u001b[0m     )\n\u001b[0;32m-> 1056\u001b[0m     outputs \u001b[39m=\u001b[39m [output \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_iterator]\n\u001b[1;32m   1057\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1058\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/pt_utils.py:111\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    110\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    112\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    113\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/pt_utils.py:112\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    111\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 112\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    113\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:983\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    981\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    982\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 983\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    984\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    985\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_classification.py:165\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/distilbert/modeling_distilbert.py:749\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 749\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    750\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    751\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    752\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    753\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    754\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    755\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    756\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    757\u001b[0m )\n\u001b[1;32m    758\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    759\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/distilbert/modeling_distilbert.py:568\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    567\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 568\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    570\u001b[0m     x\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    571\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    576\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/distilbert/modeling_distilbert.py:132\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    129\u001b[0m word_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings(input_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m embeddings \u001b[39m=\u001b[39m word_embeddings \u001b[39m+\u001b[39;49m position_embeddings  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "preds = classifier(list(test.review)) # given that 1 example takes 0.1 sec to process, 25000 will take 2500 sec or ~40 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f6168d5b204feb829be01236f94685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "432792"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = Dataset.from_dict({\n",
    "    'id': eval_ds['id'],\n",
    "    'sentiment': preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "id,sentiment\n",
      "\"\"\"12311_10\"\"\",[1]\n",
      "\"\"\"8348_2\"\"\",[1]\n",
      "\"\"\"5828_4\"\"\",[1]\n",
      "\"\"\"7186_2\"\"\",[0]\n",
      "\"\"\"12128_7\"\"\",[1]\n",
      "\"\"\"2913_8\"\"\",[1]\n",
      "\"\"\"4396_1\"\"\",[0]\n",
      "\"\"\"395_2\"\"\",[1]\n",
      "\"\"\"10616_1\"\"\",[0]\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "even such a limited model trained only on first 100 words of a sentence gives better score: Score: 0.893 over 0.84 for bag of words/RF version. It is approximately 250 position on the leaderboard fixed 8 years ago :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
